
Система извлечения и распознавания именованных сущностей (NER)
=============

Содержание
-----------------
-   [Обзор проекта](#обзор-проекта)
-   [Архитектура решения](#архитектура-решения)
-   [Выбор моделей](#выбор-моделей)
-   [Подробное описание компонентов](#подробное-описание-компонентов)
-   [Streamlit интерфейс](#streamlit-интерфейс)
-   [Запуск проекта](#запуск-проекта)
-   [Сравнение моделей](#сравнение-моделей)
-   [Post Scriptum](#post-scriptum)

Обзор проекта
-----------------
Проект представляет собой комплексное решение для:
1. Извлечения текстового контента с веб-страниц
2. Автоматической разметки именованных сущностей (NER)
3. Адаптации языковых моделей для задач NER
4. Обучения и оценки моделей на извлеченных данных

Архитектура решения
-----------------
```
[Веб-страница] → [Извлечение текста] → [Предразметка BERT] → [Обучение] → [Прогнозирование]
                     ↑                      ↑                      ↑
                [Очистка HTML]      [Мультиязычная NER]    [Кастомизированная NER]
```

Выбор моделей
-----------------
Основная модель: Sberbank AI (ruBert-large)
**Обоснование выбора:**
1. **Оптимизация для русского языка**: Специально обучена на русскоязычных данных
2. **Эффективность**: Оптимальное соотношение качества и производительности
3. **Поддержка**: Активно развивается российским разработчиком

**Кастомизация для NER:**
1. Добавлен классификационный head для 9 классов сущностей
2. Настроены соответствия меток:
   ```python
   {
       0: 'O', 1: 'B-PER', 2: 'I-PER',
       3: 'B-ORG', 4: 'I-ORG',
       5: 'B-LOC', 6: 'I-LOC',
       7: 'B-MISC', 8: 'I-MISC'
   }
   ```
3. Заморожены базовые слои для сохранения языковых знаний

Почему не выбрали Qwen:
1. **Избыточность**: Qwen слишком большая для задачи NER
2. **Ресурсоемкость**: Требует значительных вычислительных мощностей
3. **Специализация**: ruBert лучше адаптирована для русского языка

BERT модель (Davlan/bert-base-multilingual-cased-ner-hrl)
**Роль в проекте:**
- Предварительная разметка данных (teacher model)
- Мультиязычная поддержка
- Быстрое прототипирование

Подробное описание компонентов
-----------------
1. Извлечение текста.
**Функционал:**
- Имитация браузера через User-Agent
- Удаление некритичных HTML-элементов
- Интеллектуальное извлечение контента
- Очистка и нормализация текста

**Алгоритм:**
1) Запрос страницы с таймаутом
2) Парсинг BeautifulSoup
3) Удаление скриптов, стилей и др.
4) Извлечение из содержательных тегов (article, main и др.)
5) Объединение и очистка текста

2. Аннотация BERT (`annotate_with_bert`).
Функция принимает текст и возвращает список аннотаций (т. е. список распознанных сущностей ) в BIO-формате и статистику по типам сущностей (PER, ORG, LOC, MISC).

Загружается предобученный токенайзер и модель NER для создания аннотаций. Мы выбрали именно такой способ создания аннотаций, он автоматизированный, сам разбивает слова на подтокены, сам же соединяет потом части слов, сохраняет регистр, что важно для имен собственных. Самое важное – токенайзер обучен корректно обрабатывать кириллицу. Био-разметка привязана к конкретной схеме токенизации и если использовать другой токенайзер, метки будут рассинхронизированы.

Модель обрабатывает текст и возвращает «сырые» аннотации. Каждый их элемент содержит токен – само слово, метку в био-формате, позицию в начале текста, позицию в конце текста и уверенность модели.

Особенности реализации:
1)	Работа с частями слов: Объединяет токены типа ##пятый в целые слова
2)	BIO-формат: Сохраняет оригинальные метки (B-, I-, O)
3)	Ограничение длины: Берёт только первые 512 символов (ограничение BERT)
4)	Токенизация: Использует BertTokenizerFast для быстрой обработки


3. Адаптация модели (`adapt_qwen_for_ner`).
Функция подготавливает модель для распознавания именованных сущностей (NER), используя предобученную русскоязычную модель sberbank-ai/ruRoberta-large и BIO-разметку для классификации токенов.

Добавляется слой классификации с 9 выходами (BIO-метки), задается точное соответствие между метками и их числовыми id.

Фича-экстрактор замораживает все слои, кроме последнего классификационного, чтобы потом ускорить обучение и предотвратить переобучение. Это важно, поскольку предобученные веса уже содержат лингвистические знания, поэтому дообучается только «голова» для NER.

На выходе получаем готовую к обучению NER-модель.


4. NERDataset.
**Особенности:**
- Преобразование текста и аннотаций в формат для обучения
- Выравнивание меток с токенизированным текстом
- Поддержка динамической длины текста


5. Подготовка данных к обучению (prepare_training_data).
Функция подготавливает данные для обучения модели ( которую адаптировали под NER в adapt_qwen_for_ner) на основе исходного текста, аннотаций от BERT-модели ( из annotate_with_bert) и токенизатора (который создавался вместе с моделью).

Создает структуру данных для NERDataset (который используется в train_qwen).

Пошаговая работа функции:
1)	Разбивает текст на предложения
2)	Приводит аннотации к единому формату, обеспечивает совместимость с BIO-разметкой, которую ожидает модель.
3)	Корректирует позиции аннотаций относительно начала предложения, сохраняет позиции символов для точного сопоставления.
4)	Анализ баланса классов для создания взвешенной функции потерь, печатает распределение меток
5)	Подготовка данных для обучения (используется в train_qwen)

Этот код - ключевое звено между аннотацией данных и обучением модели, которое обеспечивает их совместимость.


6. Предсказание (predict_with_qwen).
На вход функция принимает текст для анализа, обученную модель NER, адаптированную через adapt_qwen_for_ner, и токенизатор, соответствующий модели. Возвращает список кортежей слово – метка_сущности.

Текст предварительно еще очищается, исправляются какие-то кодировки. Токенизация производится для текста до 512 токенов, паддинг дополняет текст до максимальной длины токенов.

Для экономии памяти при получении предсказаний модели отключается расчет градиентов. Получаем тензор с предсказанными метками для каждого токена.

Конвертируем числовые id токенов обратно в текст и собираем слова из поддтокенов. Здесь пропускаются служебные поддтокены, токены с двойной решеткой добавляют части слов к текущему слову, для обычных токенов предыдущее слово сохраняется и начинается новое.


Streamlit интерфейс
-----------------
Ключевые компоненты:
```python
# Настройки страницы
st.set_page_config(
    page_title="NER со SBER AI",
    layout="wide"
)

# Основные функции
@st.cache_data  # Кэширование данных
@st.cache_resource  # Кэширование моделей
```

Функционал:
1. Ввод URL через сайдбар
2. Настройка параметров анализа
3. Визуализация результатов:
   - Очищенный текст
   - Токены и предсказания
   - Технические метрики

Запуск проекта
-----------------
1. Установите зависимости:
```bash
pip install torch transformers beautifulsoup4 requests streamlit
```

2. Запустите приложение (если через google colab):
```bash
!streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]
```

3. Откройте в браузере:
```
http://localhost:8501
```

Сравнение моделей
-----------------
| Модель                   | Преимущества            | Недостатки                      |
|--------------------------|-------------------------|---------------------------------|
| **Sberbank ruBert**      | Лучшее качество для RU  | Только русский язык             |
| **Facebook XLM-RoBERTa** | Мультиязычность         | Требует индивидуальной настройки|
| **Babelscape WikiNEuRal**| Специализирована для NER| Ограниченный набор сущностей    |
| **Qwen**                 | Мощная языковая модель  | Избыточна для NER               |

Post Scriptum
-----------------
На самом деле, была большая надежда на Qwen модель, но он считался очень долго. Проблема, к сожалению, не была решена, поэтому было принято решение использовать в качестве основной модели sberbank ai.
