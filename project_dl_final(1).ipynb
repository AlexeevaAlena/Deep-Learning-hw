{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "n-C5u4NgKWah",
      "metadata": {
        "id": "n-C5u4NgKWah"
      },
      "source": [
        "# QWEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YtOJd-7kmEXw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890,
          "referenced_widgets": [
            "f1fe27e3860c4cf380107fb67d43e2f8",
            "415f1d3040fb4902aeb231047ac7fa70",
            "307a489504ef401a9ac97224e10a66bc",
            "28a71f72e8b54221a4af5863f5e8597b",
            "0b98f7417dff428fbba978a6307ced56",
            "e3be9576eebf46fba1eab3d0384a8a80",
            "4eb63505ad5d4248987bc9d1ee304069",
            "fbcd4e8cb8ab48d69d2c8f28f01ed159",
            "321ffc2d757443b48e6988788de84f6e",
            "018b70395c624efc8e80139ce7dacd21",
            "35b504a135fb4f62b5e4fa02779a8e8a"
          ]
        },
        "collapsed": true,
        "id": "YtOJd-7kmEXw",
        "outputId": "9f53d217-5cff-4289-9d85-54a1972b3114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Извлечённый текст:\n",
            "Блог / Новости Возрождение лютоволков из «Игры престолов»: научный прорыв Полина Недашковская 07 мая, 2025 • 2 мин • 155 Копировать ссылку Telegram VK WhatsApp Биотехнологическая компания Colossal Biosciences из Далласа совершила прорыв в области возрождения вымерших видов животных, воссоздав ужасных волков (dire wolves), известных широкой аудитории как «лютоволки» из «Игры престолов» . Процесс был долгим и сложным, но в итоге ученые смогли вернуть к жизни этот вид, вымерший более 12 тысяч лет н...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пример аннотаций BERT:\n",
            "[{'entity': 'B-PER', 'score': np.float32(0.9994863), 'index': 26, 'word': 'Пол', 'start': 74, 'end': 77}, {'entity': 'I-PER', 'score': np.float32(0.98204434), 'index': 27, 'word': '##ина', 'start': 77, 'end': 80}, {'entity': 'I-PER', 'score': np.float32(0.9996425), 'index': 28, 'word': 'Не', 'start': 81, 'end': 83}, {'entity': 'I-PER', 'score': np.float32(0.9996587), 'index': 29, 'word': '##да', 'start': 83, 'end': 85}, {'entity': 'I-PER', 'score': np.float32(0.9995888), 'index': 30, 'word': '##шко', 'start': 85, 'end': 88}]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1fe27e3860c4cf380107fb67d43e2f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Qwen2ForTokenClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-32B-Instruct and are newly initialized: ['score.bias', 'score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2/2 [09:35<00:00, 287.53s/it]\n",
            "Validating: 100%|██████████| 1/1 [02:52<00:00, 172.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: nan | Val Loss: nan\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2/2 [08:55<00:00, 267.70s/it]\n",
            "Validating: 100%|██████████| 1/1 [03:03<00:00, 183.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: nan | Val Loss: nan\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2/2 [09:15<00:00, 277.65s/it]\n",
            "Validating: 100%|██████████| 1/1 [02:54<00:00, 174.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: nan | Val Loss: nan\n",
            "Предсказания модели Qwen (первые 20 токенов):\n",
            "Ðĳ -> B-PER\n",
            "Ð»Ð¾Ð³ -> B-PER\n",
            "Ġ/ -> B-MISC\n",
            "ĠÐĿ -> B-PER\n",
            "Ð¾Ð² -> B-PER\n",
            "Ð¾ÑģÑĤÐ¸ -> B-PER\n",
            "ĠÐĴÐ¾Ð· -> B-PER\n",
            "ÑĢÐ¾Ð¶ -> B-MISC\n",
            "Ð´ -> B-PER\n",
            "ÐµÐ½Ð¸Ðµ -> I-PER\n",
            "ĠÐ» -> B-PER\n",
            "ÑİÑĤ -> B-PER\n",
            "Ð¾Ð² -> B-PER\n",
            "Ð¾Ð» -> B-PER\n",
            "ÐºÐ¾Ð² -> B-PER\n",
            "ĠÐ¸Ð· -> B-PER\n",
            "ĠÂ« -> I-ORG\n",
            "Ðĺ -> B-MISC\n",
            "Ð³ -> B-PER\n",
            "ÑĢÑĭ -> B-MISC\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, BertTokenizerFast, BertForTokenClassification\n",
        "from transformers import pipeline\n",
        "from typing import List, Dict, Tuple\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Конфигурация\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "QWEN_MODEL_NAME =  \"Qwen/Qwen2.5-32B-Instruct\"\n",
        "BERT_MODEL_NAME = \"Davlan/bert-base-multilingual-cased-ner-hrl\" # Предобученная модель для NER\n",
        "\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "import requests\n",
        "\n",
        "def extract_text_from_url(url: str, timeout: int = 10) -> str:\n",
        "    \"\"\"\n",
        "    Извлекает основной текстовый контент с веб-страницы по URL.\n",
        "\n",
        "    Параметры:\n",
        "        url (str): URL веб-страницы\n",
        "        timeout (int): Таймаут запроса в секундах (по умолчанию 10)\n",
        "\n",
        "    Возвращает:\n",
        "        str: Очищенный текстовый контент страницы\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Заголовки для имитации браузера\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        # Запрос с таймаутом и заголовками\n",
        "        response = requests.get(url, headers=headers, timeout=timeout)\n",
        "        response.raise_for_status()  # Проверка на ошибки HTTP\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Удаление нежелательных элементов\n",
        "        for element in soup(['script', 'style', 'nav', 'footer', 'iframe', 'noscript', 'svg']):\n",
        "            element.decompose()\n",
        "\n",
        "        # Удаление HTML-комментариев\n",
        "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
        "            comment.extract()\n",
        "\n",
        "        # Извлечение текста из основных содержательных тегов\n",
        "        text_elements = []\n",
        "        for tag in ['article', 'main', 'div', 'section', 'p']:\n",
        "            elements = soup.find_all(tag)\n",
        "            for element in elements:\n",
        "                # Проверка на содержание значимого текста\n",
        "                if len(element.get_text(strip=True)) > 50:  # Минимум 50 символов\n",
        "                    text_elements.append(element.get_text(separator=' ', strip=True))\n",
        "\n",
        "        # Если не нашли достаточно текста в специфичных тегах, берем весь body\n",
        "        if not text_elements or sum(len(t) for t in text_elements) < 500:\n",
        "            text_elements = [soup.get_text(separator=' ', strip=True)]\n",
        "\n",
        "        # Объединение и очистка текста\n",
        "        text = ' '.join(text_elements)\n",
        "\n",
        "        # Удаление лишних пробелов и переносов строк\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Ошибка при запросе к {url}: {str(e)}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Неожиданная ошибка при обработке {url}: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def annotate_with_bert(text: str) -> List[Dict]:\n",
        "    \"\"\"Аннотирует текст с помощью предобученной модели BERT.\"\"\"\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL_NAME)\n",
        "    model = BertForTokenClassification.from_pretrained(BERT_MODEL_NAME).to(DEVICE)\n",
        "\n",
        "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=DEVICE)\n",
        "    annotations = nlp(text)\n",
        "    return annotations\n",
        "\n",
        "def adapt_qwen_for_ner(model_name: str = QWEN_MODEL_NAME):\n",
        "    \"\"\"Загружает модель Qwen и адаптирует её для NER.\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=9,  # Стандартное количество классов для NER (PER, ORG, LOC и т.д.)\n",
        "        id2label={0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG',\n",
        "                  5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'},\n",
        "        label2id={'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4,\n",
        "                  'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Замораживаем все слои, кроме последнего\n",
        "    for param in model.base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    \"\"\"Кастомный Dataset для NER.\"\"\"\n",
        "    def __init__(self, texts, annotations, tokenizer, model_config, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.annotations = annotations\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model_config = model_config  # Добавляем конфигурацию модели\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        annotations = self.annotations[idx]\n",
        "\n",
        "        # Токенизация с выравниванием меток\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt',\n",
        "            return_offsets_mapping=True\n",
        "        )\n",
        "\n",
        "        # Создаем массив меток, заполненный -100 (игнорируется при вычислении потерь)\n",
        "        labels = torch.full((self.max_length,), -100, dtype=torch.long)\n",
        "\n",
        "        # Преобразуем аннотации в метки для токенов\n",
        "        offset_mapping = encoding['offset_mapping'][0]\n",
        "        for ann in annotations:\n",
        "            start, end = ann['start'], ann['end']\n",
        "            label = ann['entity']\n",
        "\n",
        "            # Находим токены, которые пересекаются с аннотацией\n",
        "            for i, (token_start, token_end) in enumerate(offset_mapping):\n",
        "                if token_start >= end or token_end <= start:\n",
        "                    continue\n",
        "                if token_start != 0 or token_end != 0:  # Игнорируем специальные токены\n",
        "                    labels[i] = self.model_config.label2id.get(label, 0)  # Используем self.model_config\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "def prepare_training_data(text, bert_annotations, tokenizer, test_size=0.2):\n",
        "    \"\"\"Подготавливает данные для обучения и валидации.\"\"\"\n",
        "    # Разделяем текст на предложения (упрощенно)\n",
        "    sentences = [sent.strip() for sent in text.split('.') if len(sent.strip()) > 0]\n",
        "\n",
        "    # Сопоставляем аннотации с предложениями (упрощенно)\n",
        "    sentence_annotations = []\n",
        "    current_pos = 0\n",
        "    for sent in sentences:\n",
        "        sent_len = len(sent)\n",
        "        sent_anns = []\n",
        "        for ann in bert_annotations:\n",
        "            if current_pos <= ann['start'] < current_pos + sent_len:\n",
        "                adjusted_ann = {\n",
        "                    'start': ann['start'] - current_pos,\n",
        "                    'end': ann['end'] - current_pos,\n",
        "                    'entity': ann['entity']\n",
        "                }\n",
        "                sent_anns.append(adjusted_ann)\n",
        "        sentence_annotations.append(sent_anns)\n",
        "        current_pos += sent_len + 1  # +1 для точки\n",
        "\n",
        "    # Разделяем на train и test\n",
        "    split_idx = int(len(sentences) * (1 - test_size))\n",
        "    train_texts, train_anns = sentences[:split_idx], sentence_annotations[:split_idx]\n",
        "    val_texts, val_anns = sentences[split_idx:], sentence_annotations[split_idx:]\n",
        "\n",
        "    return train_texts, train_anns, val_texts, val_anns\n",
        "\n",
        "def train_qwen(model, tokenizer, text, bert_annotations, epochs=3, batch_size=8, learning_rate=2e-5):\n",
        "    \"\"\"Полноценная функция обучения модели Qwen для NER.\"\"\"\n",
        "\n",
        "    # Подготовка данных\n",
        "    train_texts, train_anns, val_texts, val_anns = prepare_training_data(text, bert_annotations, tokenizer)\n",
        "\n",
        "    train_dataset = NERDataset(train_texts, train_anns, tokenizer, model_config=model.config)\n",
        "    val_dataset = NERDataset(val_texts, val_anns, tokenizer, model_config=model.config)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Настройка оптимизатора и планировщика\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Обучение\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        # Фаза обучения\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                labels=batch['labels']\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Фаза валидации\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    input_ids=batch['input_ids'],\n",
        "                    attention_mask=batch['attention_mask'],\n",
        "                    labels=batch['labels']\n",
        "                )\n",
        "\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "            # Собираем предсказания и метки для метрик\n",
        "            preds = torch.argmax(outputs.logits, dim=2)\n",
        "            mask = batch['labels'] != -100  # Игнорируем метки -100\n",
        "\n",
        "            all_preds.extend(preds[mask].cpu().numpy())\n",
        "            all_labels.extend(batch['labels'][mask].cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        # Выводим метрики\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "        # print(classification_report(\n",
        "        #     all_labels,\n",
        "        #     all_preds,\n",
        "        #     target_names=list(model.config.id2label.values()),\n",
        "        #     zero_division=0\n",
        "        # ))\n",
        "\n",
        "        # Сохраняем лучшую модель\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            os.makedirs(\"./qwen_ner\", exist_ok=True)\n",
        "            model.save_pretrained(\"./qwen_ner\")\n",
        "            tokenizer.save_pretrained(\"./qwen_ner\")\n",
        "            print(\"Saved best model!\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def predict_with_qwen(text: str, model, tokenizer) -> List[Tuple[str, str]]:\n",
        "    \"\"\"Делает предсказания с помощью адаптированной модели Qwen.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    predictions = torch.argmax(outputs.logits, dim=2)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    labels = [model.config.id2label[p.item()] for p in predictions[0]]\n",
        "\n",
        "    return list(zip(tokens, labels))\n",
        "\n",
        "def main(url: str):\n",
        "    # 1. Извлекаем текст с веб-страницы\n",
        "    text = extract_text_from_url(url)\n",
        "    print(f\"Извлечённый текст:\\n{text[:500]}...\\n\")\n",
        "\n",
        "    # 2. Аннотируем с помощью BERT\n",
        "    bert_annotations = annotate_with_bert(text[:512])  # Ограничиваем длину для BERT\n",
        "    print(f\"Пример аннотаций BERT:\\n{bert_annotations[:5]}\\n\")\n",
        "\n",
        "    # 3. Адаптируем Qwen для NER\n",
        "    qwen_model, qwen_tokenizer = adapt_qwen_for_ner()\n",
        "\n",
        "    # 4. Обучаем модель\n",
        "    qwen_model = train_qwen(qwen_model, qwen_tokenizer, text[:2000], bert_annotations, epochs=3)\n",
        "\n",
        "    # 5. Делаем предсказания\n",
        "    predictions = predict_with_qwen(text[:512], qwen_model, qwen_tokenizer)\n",
        "\n",
        "    # 6. Выводим результаты\n",
        "    print(\"Предсказания модели Qwen (первые 20 токенов):\")\n",
        "    for token, label in predictions[:20]:\n",
        "        print(f\"{token} -> {label}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://amulex.ru/daily/news/vozrozhdenie-lyutovolkov-iz-igry-prestolov-nauchnyj-proryv-bjfj4kf/?ysclid=mae5l3wify323318294\"  # Пример URL\n",
        "    main(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9SmfpAamSL30",
      "metadata": {
        "id": "9SmfpAamSL30"
      },
      "source": [
        "# SBERBANK AI - основная модель (все остальные - экспериментальные наработки)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "NzDqam4ASLci",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzDqam4ASLci",
        "outputId": "b707410c-0d8d-497d-f8ef-d8cb04d53bee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пробуем загрузить DeepPavlov/bert-base-cased-conversational-ner (Русская NER (рекомендуется))...\n",
            "Ошибка загрузки DeepPavlov/bert-base-cased-conversational-ner: DeepPavlov/bert-base-cased-conversational-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "Пробуем загрузить Davlan/bert-base-multilingual-cased-ner-hrl (Многоязычная NER)...\n",
            "Успешно! Используем Davlan/bert-base-multilingual-cased-ner-hrl\n",
            "\n",
            "=== Исходный текст ===\n",
            " История Google Материал из Википедии — свободной энциклопедии Перейти к навигации Перейти к поиску Компания Google официально была создана в 1998 году. Содержание 1 1996—2001 1.1 Создание поиска 1.2 Создание компании 2 2002—2015 3 2015—2019 4 См. также 5 Примечания 6 Ссылки 1996—2001 [ править | править код ] Создание поиска [ править | править код ] Сергей Брин Компания Google появилась как развитие научного проекта Ларри Пейджа и Сергея Брина . В 1996 году студенты Стэнфорда работали над Стэнф \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Примеры аннотаций ===\n",
            "Google -> B-ORG (score: 0.99)\n",
            "Сергей -> B-PER (score: 1.00)\n",
            "Брин -> I-PER (score: 1.00)\n",
            "Google -> B-ORG (score: 1.00)\n",
            "Ларри -> B-PER (score: 1.00)\n",
            "\n",
            "=== Распределение сущностей ===\n",
            "PER: 6\n",
            "ORG: 4\n",
            "LOC: 1\n",
            "MISC: 0\n",
            "\n",
            "=== BIO-распределение ===\n",
            "B-PER: 3\n",
            "I-PER: 3\n",
            "B-ORG: 3\n",
            "I-ORG: 1\n",
            "B-LOC: 1\n",
            "I-LOC: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Распределение классов ===\n",
            "B-PER: 3 (27.3%)\n",
            "I-PER: 3 (27.3%)\n",
            "B-ORG: 3 (27.3%)\n",
            "I-ORG: 1 (9.1%)\n",
            "B-LOC: 1 (9.1%)\n",
            "I-LOC: 0 (0.0%)\n",
            "B-MISC: 0 (0.0%)\n",
            "I-MISC: 0 (0.0%)\n",
            "O: 0 (0.0%)\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:17<00:00, 17.53s/it]\n",
            "Validating: 100%|██████████| 1/1 [00:05<00:00,  5.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.2937 | Val Loss: nan\n",
            "Нет данных для оценки метрик (возможно, все метки - 'O')\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:17<00:00, 17.32s/it]\n",
            "Validating: 100%|██████████| 1/1 [00:04<00:00,  4.93s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.3845 | Val Loss: nan\n",
            "Нет данных для оценки метрик (возможно, все метки - 'O')\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:16<00:00, 16.40s/it]\n",
            "Validating: 100%|██████████| 1/1 [00:06<00:00,  6.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.1666 | Val Loss: nan\n",
            "Нет данных для оценки метрик (возможно, все метки - 'O')\n",
            "\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:16<00:00, 16.22s/it]\n",
            "Validating: 100%|██████████| 1/1 [00:05<00:00,  5.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.1931 | Val Loss: nan\n",
            "Нет данных для оценки метрик (возможно, все метки - 'O')\n",
            "\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:16<00:00, 16.75s/it]\n",
            "Validating: 100%|██████████| 1/1 [00:04<00:00,  4.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.3689 | Val Loss: nan\n",
            "Нет данных для оценки метрик (возможно, все метки - 'O')\n",
            "\n",
            "=== Результаты распознавания ===\n",
            "ÐĺÑģÑĤÐ¾ÑĢÐ¸Ñı Google ÐľÐ°ÑĤÐµÑĢÐ¸ Ð°Ð» Ð¸Ð· ÐĴÐ¸ÐºÐ¸ Ð¿ÐµÐ´ Ð¸Ð¸ âĢĶ ÑģÐ²Ð¾Ð±Ð¾Ð´Ð½Ð¾Ð¹ ÑįÐ½ÑĨÐ¸ÐºÐ»Ð¾Ð¿ÐµÐ´ Ð¸Ð¸ ÐŁÐµÑĢ ÐµÐ¹ ÑĤÐ¸ Ðº Ð½Ð°Ð²Ð¸Ð³ Ð°ÑĨÐ¸Ð¸ ÐŁÐµÑĢ ÐµÐ¹ ÑĤÐ¸ Ðº Ð¿Ð¾Ð¸ÑģÐº Ñĥ ÐļÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ñı Google Ð¾ÑĦÐ¸ÑĨÐ¸Ð°Ð»ÑĮÐ½Ð¾ Ð±ÑĭÐ»Ð° ÑģÐ¾Ð·Ð´Ð°Ð½Ð° Ð² 1998 Ð³Ð¾Ð´Ñĥ . Ð¡Ð¾Ð´ÐµÑĢÐ¶ Ð°Ð½Ð¸Ðµ 1 1996 âĢĶ 2001 1 . 1 Ð¡Ð¾Ð·Ð´ Ð°Ð½Ð¸Ðµ Ð¿Ð¾Ð¸ÑģÐºÐ° 1 . 2 Ð¡Ð¾Ð·Ð´ Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸ 2 2002 âĢĶ 2015 3 2015 âĢĶ 20 19 4 Ð¡Ð¼ . ÑĤÐ°ÐºÐ¶Ðµ 5 ÐŁÑĢÐ¸Ð¼ÐµÑĩ Ð°Ð½Ð¸Ñı 6 Ð¡ÑģÑĭÐ»ÐºÐ¸ 1996 âĢĶ 2001 [ Ð¿ÑĢÐ°Ð²Ð¸ÑĤÑĮ | Ð¿ÑĢÐ°Ð²Ð¸ÑĤÑĮ ÐºÐ¾Ð´  ] Ð¡Ð¾Ð·Ð´ Ð°Ð½Ð¸Ðµ Ð¿Ð¾Ð¸ÑģÐºÐ° [ Ð¿ÑĢÐ°Ð²Ð¸ÑĤÑĮ | Ð¿ÑĢÐ°Ð²Ð¸ÑĤÑĮ ÐºÐ¾Ð´  ] Ð¡ÐµÑĢÐ³ÐµÐ¹ ÐĳÑĢ Ð¸Ð½ ÐļÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ñı Google Ð¿Ð¾ÑıÐ²Ð¸Ð»Ð°ÑģÑĮ ÐºÐ°Ðº ÑĢÐ°Ð·Ð²Ð¸ÑĤÐ¸Ðµ Ð½Ð°ÑĥÑĩÐ½Ð¾Ð³Ð¾ Ð¿ÑĢÐ¾ÐµÐºÑĤÐ° ÐĽÐ°ÑĢ ÑĢÐ¸ ÐŁÐµÐ¹ Ð´Ð¶Ð° Ð¸ Ð¡ÐµÑĢÐ³ÐµÑı ÐĳÑĢ Ð¸Ð½Ð° . ÐĴ 1996 Ð³Ð¾Ð´Ñĥ ÑģÑĤÑĥÐ´ÐµÐ½ÑĤÑĭ Ð¡ÑĤÑįÐ½ ÑĦÐ¾ÑĢÐ´Ð° ÑĢÐ°Ð±Ð¾ÑĤÐ°Ð»Ð¸ Ð½Ð°Ð´ Ð¡ÑĤÑįÐ½ ÑĦÐ¾ÑĢÐ´ ÑģÐºÐ¸Ð¼ ÐŁÑĢ Ð¾Ðµ \n",
            "\n",
            "\n",
            "=== Детализация сущностей ===\n",
            "ÐĺÑģÑĤÐ¾ÑĢÐ¸Ñı -> B-MISC\n",
            "Google -> I-MISC\n",
            "ÐľÐ°ÑĤÐµÑĢÐ¸ -> I-MISC\n",
            "Ð°Ð» -> I-MISC\n",
            "Ð¸Ð¸ -> B-MISC\n",
            "âĢĶ -> I-MISC\n",
            "ÑģÐ²Ð¾Ð±Ð¾Ð´Ð½Ð¾Ð¹ -> I-MISC\n",
            "ÑįÐ½ÑĨÐ¸ÐºÐ»Ð¾Ð¿ÐµÐ´ -> I-MISC\n",
            "Ð¸Ð¸ -> B-MISC\n",
            "ÐŁÐµÑĢ -> I-MISC\n",
            "ÐµÐ¹ -> I-MISC\n",
            "ÑĤÐ¸ -> I-MISC\n",
            "Ðº -> B-MISC\n",
            "Ð°ÑĨÐ¸Ð¸ -> B-MISC\n",
            "ÐŁÐµÑĢ -> I-MISC\n",
            "ÐµÐ¹ -> I-MISC\n",
            "ÑĤÐ¸ -> I-MISC\n",
            "Ð¿Ð¾Ð¸ÑģÐº -> B-MISC\n",
            "Ñĥ -> I-MISC\n",
            "ÐļÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ñı -> I-MISC\n",
            "Google -> I-MISC\n",
            "Ð¾ÑĦÐ¸ÑĨÐ¸Ð°Ð»ÑĮÐ½Ð¾ -> I-MISC\n",
            "Ð±ÑĭÐ»Ð° -> B-ORG\n",
            "1998 -> B-PER\n",
            "Ð³Ð¾Ð´Ñĥ -> B-MISC\n",
            "Ð°Ð½Ð¸Ðµ -> B-ORG\n",
            "1 -> B-MISC\n",
            "1996 -> B-MISC\n",
            "2001 -> B-MISC\n",
            "1 -> B-MISC\n",
            ". -> B-LOC\n",
            "1 -> B-MISC\n",
            "Ð¡Ð¾Ð·Ð´ -> B-ORG\n",
            "Ð¿Ð¾Ð¸ÑģÐºÐ° -> B-MISC\n",
            "1 -> B-MISC\n",
            ". -> B-LOC\n",
            "2 -> B-MISC\n",
            "Ð¡Ð¾Ð·Ð´ -> B-ORG\n",
            "Ð°Ð½Ð¸Ðµ -> B-ORG\n",
            "2002 -> B-MISC\n",
            "3 -> B-MISC\n",
            "2015 -> B-PER\n",
            "20 -> B-LOC\n",
            "19 -> B-LOC\n",
            "Ð°Ð½Ð¸Ñı -> B-ORG\n",
            "1996 -> B-MISC\n",
            "2001 -> B-PER\n",
            " -> B-MISC\n",
            "] -> B-LOC\n",
            "Ð¿Ð¾Ð¸ÑģÐºÐ° -> B-MISC\n",
            "[ -> B-ORG\n",
            "Ð¿ÑĢÐ°Ð²Ð¸ÑĤÑĮ -> B-LOC\n",
            "Ð¿ÑĢÐ°Ð²Ð¸ÑĤÑĮ -> B-LOC\n",
            " -> B-MISC\n",
            "] -> B-LOC\n",
            "ÐĳÑĢ -> B-MISC\n",
            "Ð¸Ð½ -> I-MISC\n",
            "ÐļÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ñı -> I-MISC\n",
            "Google -> I-MISC\n",
            "Ð¿Ð¾ÑıÐ²Ð¸Ð»Ð°ÑģÑĮ -> B-ORG\n",
            "ÑĢÐ¸ -> B-ORG\n",
            "Ð¸ -> B-MISC\n",
            "Ð¡ÐµÑĢÐ³ÐµÑı -> I-MISC\n",
            "ÐĳÑĢ -> B-MISC\n",
            "Ð¸Ð½Ð° -> B-ORG\n",
            "ÐĴ -> B-LOC\n",
            "1996 -> B-MISC\n",
            "Ð³Ð¾Ð´Ñĥ -> B-PER\n",
            "Ð¡ÑĤÑįÐ½ -> B-PER\n",
            "ÑĦÐ¾ÑĢÐ´Ð° -> B-PER\n",
            "Ð½Ð°Ð´ -> B-ORG\n",
            "Ð¡ÑĤÑįÐ½ -> B-PER\n",
            "ÑĦÐ¾ÑĢÐ´ -> B-PER\n",
            "ÑģÐºÐ¸Ð¼ -> B-PER\n",
            "ÐŁÑĢ -> I-PER\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, BertTokenizerFast, BertForTokenClassification\n",
        "from transformers import pipeline\n",
        "from typing import List, Dict, Tuple\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "import requests\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "\n",
        "# Конфигурация\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "QWEN_MODEL_NAME =  \"sberbank-ai/ruRoberta-large\"\n",
        "PUBLIC_MODELS = {\n",
        "    \"DeepPavlov/bert-base-cased-conversational-ner\": \"Русская NER (рекомендуется)\",\n",
        "    \"Davlan/bert-base-multilingual-cased-ner-hrl\": \"Многоязычная NER\"\n",
        "}\n",
        "\n",
        "# Автоматический выбор рабочей модели\n",
        "for model_name, desc in PUBLIC_MODELS.items():\n",
        "    try:\n",
        "        print(f\"Пробуем загрузить {model_name} ({desc})...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "        BERT_MODEL_NAME = model_name\n",
        "        print(f\"Успешно! Используем {model_name}\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка загрузки {model_name}: {str(e)}\")\n",
        "else:\n",
        "    raise ValueError(\"Не удалось загрузить ни одну публичную модель!\")\n",
        "\n",
        "BERT_MODEL_NAME = \"Davlan/bert-base-multilingual-cased-ner-hrl\"  # Предобученная BERT для NER\n",
        "\n",
        "\n",
        "def is_bad_encoding(text: str) -> bool:\n",
        "    return any(c in text for c in ('Ð', 'Ñ', 'â'))\n",
        "\n",
        "def fix_encoding(text: str) -> str:\n",
        "    try:\n",
        "        return text.encode('latin1').decode('utf-8')\n",
        "    except:\n",
        "        try:\n",
        "            return text.encode('iso-8859-1').decode('utf-8')\n",
        "        except:\n",
        "            return text\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Очистка текста от артефактов\"\"\"\n",
        "    if is_bad_encoding(text):\n",
        "        text = fix_encoding(text)\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "\n",
        "TOKENIZER = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "\n",
        "def extract_text_from_url(url: str, timeout: int = 10) -> str:\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "        response = requests.get(url, headers=headers, timeout=timeout)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # NEW: Принудительная установка кодировки\n",
        "        response.encoding = 'utf-8'\n",
        "        text = response.text\n",
        "\n",
        "        # NEW: Проверка и исправление кодировки\n",
        "        if is_bad_encoding(text):\n",
        "            text = fix_encoding(text)\n",
        "\n",
        "        soup = BeautifulSoup(text, 'html.parser')\n",
        "\n",
        "        for element in soup(['script', 'style', 'nav', 'footer', 'iframe', 'noscript', 'svg']):\n",
        "            element.decompose()\n",
        "\n",
        "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
        "            comment.extract()\n",
        "\n",
        "        text_elements = []\n",
        "        for tag in ['article', 'main', 'div', 'section', 'p']:\n",
        "            elements = soup.find_all(tag)\n",
        "            for element in elements:\n",
        "                if len(element.get_text(strip=True)) > 50:\n",
        "                    text_elements.append(element.get_text(separator=' ', strip=True))\n",
        "\n",
        "        if not text_elements or sum(len(t) for t in text_elements) < 500:\n",
        "            text_elements = [soup.get_text(separator=' ', strip=True)]\n",
        "\n",
        "        text = ' '.join(text_elements)\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при обработке URL: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def annotate_with_bert(text: str) -> Tuple[List[Dict], Dict[str, int]]:\n",
        "    try:\n",
        "        tokenizer = BertTokenizerFast.from_pretrained(\n",
        "            BERT_MODEL_NAME,\n",
        "            do_lower_case=False,\n",
        "            strip_accents=False\n",
        "        )\n",
        "        model = AutoModelForTokenClassification.from_pretrained(BERT_MODEL_NAME).to(DEVICE)\n",
        "\n",
        "        nlp = pipeline(\"ner\", model=model, tokenizer=TOKENIZER,\n",
        "                      device=DEVICE, aggregation_strategy=None)  # Убрали агрегацию\n",
        "\n",
        "        raw_annotations = nlp(text[:512])\n",
        "\n",
        "        # Конвертация в BIO-формат с сохранением I-меток\n",
        "        annotations = []\n",
        "        for i, ann in enumerate(raw_annotations):\n",
        "            entity = ann['entity']\n",
        "            # Обработка частей слов\n",
        "            if ann['word'].startswith('##'):\n",
        "                if annotations:\n",
        "                    annotations[-1]['word'] += ann['word'][2:]\n",
        "                    annotations[-1]['end'] = ann['end']\n",
        "                continue\n",
        "\n",
        "            new_ann = {\n",
        "                'start': ann['start'],\n",
        "                'end': ann['end'],\n",
        "                'entity': entity,  # Сохраняем оригинальные метки (уже в BIO)\n",
        "                'word': ann['word'],\n",
        "                'score': ann['score']\n",
        "            }\n",
        "            annotations.append(new_ann)\n",
        "\n",
        "        # Статистика по типам сущностей\n",
        "        type_counts = {'PER': 0, 'ORG': 0, 'LOC': 0, 'MISC': 0}\n",
        "        for ann in annotations:\n",
        "            if ann['entity'] != 'O':\n",
        "                ent_type = ann['entity'].split('-')[-1]\n",
        "                type_counts[ent_type] = type_counts.get(ent_type, 0) + 1\n",
        "\n",
        "        return annotations, type_counts\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка аннотации: {str(e)}\")\n",
        "        return [], {'PER': 0, 'ORG': 0, 'LOC': 0, 'MISC': 0}\n",
        "\n",
        "def adapt_qwen_for_ner(model_name: str = QWEN_MODEL_NAME):\n",
        "    \"\"\"Адаптирует русский BERT для NER.\"\"\"\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # Явно задаем метки BIO-формата\n",
        "        label2id = {\n",
        "            'O': 0,\n",
        "            'B-PER': 1, 'I-PER': 2,\n",
        "            'B-ORG': 3, 'I-ORG': 4,\n",
        "            'B-LOC': 5, 'I-LOC': 6,\n",
        "            'B-MISC': 7, 'I-MISC': 8\n",
        "        }\n",
        "\n",
        "        model = AutoModelForTokenClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=len(label2id),\n",
        "            id2label={i: l for l, i in label2id.items()},\n",
        "            label2id=label2id\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Замораживаем все слои, кроме последнего\n",
        "        for param in model.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке модели: {str(e)}\")\n",
        "        return None, None  # Возвращаем явные None при ошибке\n",
        "\n",
        "def analyze_class_balance(annotations: List[Dict]) -> Dict[str, int]:\n",
        "    \"\"\"Анализ распределения меток с гарантированным возвратом всех типов\"\"\"\n",
        "    bio_labels = ['B-PER', 'I-PER', 'B-ORG', 'I-ORG',\n",
        "                 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC', 'O']\n",
        "    counts = {label: 0 for label in bio_labels}\n",
        "\n",
        "    for ann in annotations:\n",
        "        label = ann.get('entity', 'O')\n",
        "        if label in counts:\n",
        "            counts[label] += 1\n",
        "        else:\n",
        "            # Конвертируем неизвестные метки\n",
        "            if label.startswith('B-'):\n",
        "                counts['B-MISC'] += 1\n",
        "            elif label.startswith('I-'):\n",
        "                counts['I-MISC'] += 1\n",
        "            else:\n",
        "                counts['O'] += 1\n",
        "\n",
        "    return counts\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    \"\"\"Кастомный Dataset для NER.\"\"\"\n",
        "    def __init__(self, texts, annotations, tokenizer, model_config, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.annotations = annotations\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model_config = model_config  # Добавляем конфигурацию модели\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        annotations = self.annotations[idx]\n",
        "\n",
        "        # Токенизация с выравниванием меток\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt',\n",
        "            return_offsets_mapping=True\n",
        "        )\n",
        "\n",
        "        # Создаем массив меток, заполненный -100 (игнорируется при вычислении потерь)\n",
        "        labels = torch.full((self.max_length,), -100, dtype=torch.long)\n",
        "\n",
        "        # Преобразуем аннотации в метки для токенов\n",
        "        offset_mapping = encoding['offset_mapping'][0]\n",
        "        for ann in annotations:\n",
        "            start, end = ann['start'], ann['end']\n",
        "            label = ann['entity']\n",
        "\n",
        "            # Находим токены, которые пересекаются с аннотацией\n",
        "            for i, (token_start, token_end) in enumerate(offset_mapping):\n",
        "                if token_start >= end or token_end <= start:\n",
        "                    continue\n",
        "                if token_start != 0 or token_end != 0:  # Игнорируем специальные токены\n",
        "                    labels[i] = self.model_config.label2id.get(label, 0)  # Используем self.model_config\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "def prepare_training_data(text, bert_annotations, tokenizer, test_size=0.2):\n",
        "    \"\"\"Подготавливает данные для обучения и валидации.\"\"\"\n",
        "    # Разделяем текст на предложения (упрощенно)\n",
        "    sentences = [sent.strip() for sent in text.split('.') if len(sent.strip()) > 0]\n",
        "\n",
        "    # Модифицируем аннотации для BIO-формата\n",
        "    for ann in bert_annotations:\n",
        "        if isinstance(ann, dict) and 'entity_group' in ann:\n",
        "            ann['entity'] = f\"B-{ann['entity_group']}\"  # Конвертируем в BIO-формат\n",
        "        elif isinstance(ann, dict):\n",
        "            ann['entity'] = ann.get('entity', 'O')  # Добавляем поле entity, если его нет\n",
        "\n",
        "    # Сопоставляем аннотации с предложениями\n",
        "    sentence_annotations = []\n",
        "    current_pos = 0\n",
        "    for sent in sentences:\n",
        "        sent_len = len(sent)\n",
        "        sent_anns = []\n",
        "        for ann in bert_annotations:\n",
        "            if isinstance(ann, dict) and 'start' in ann and 'end' in ann:\n",
        "                if current_pos <= ann['start'] < current_pos + sent_len:\n",
        "                    adjusted_ann = {\n",
        "                        'start': ann['start'] - current_pos,\n",
        "                        'end': ann['end'] - current_pos,\n",
        "                        'entity': ann['entity']  # Используем уже преобразованную метку\n",
        "                    }\n",
        "                    sent_anns.append(adjusted_ann)\n",
        "        sentence_annotations.append(sent_anns)\n",
        "        current_pos += sent_len + 1\n",
        "\n",
        "    # Анализ баланса классов\n",
        "    all_annotations = [ann for sent_anns in sentence_annotations for ann in sent_anns]\n",
        "    class_counts = analyze_class_balance(all_annotations)\n",
        "\n",
        "    print(\"\\n=== Распределение классов ===\")\n",
        "    for cls, count in class_counts.items():\n",
        "        print(f\"{cls}: {count} ({count/len(all_annotations):.1%})\" if len(all_annotations) > 0 else f\"{cls}: 0\")\n",
        "\n",
        "    # Разделяем на train и test\n",
        "    split_idx = int(len(sentences) * (1 - test_size))\n",
        "    train_texts, train_anns = sentences[:split_idx], sentence_annotations[:split_idx]\n",
        "    val_texts, val_anns = sentences[split_idx:], sentence_annotations[split_idx:]\n",
        "\n",
        "    return train_texts, train_anns, val_texts, val_anns\n",
        "\n",
        "def train_qwen(model, tokenizer, text, bert_annotations, epochs=10, batch_size=32, learning_rate=2e-5):\n",
        "    \"\"\"Полноценная функция обучения модели sberbank ai для NER.\"\"\"\n",
        "\n",
        "    # Подготовка данных\n",
        "    train_texts, train_anns, val_texts, val_anns = prepare_training_data(text, bert_annotations, tokenizer)\n",
        "\n",
        "    # Создание взвешенной функции потерь\n",
        "    all_annotations = [ann for sent_anns in train_anns + val_anns for ann in sent_anns]\n",
        "    class_counts = analyze_class_balance(all_annotations)\n",
        "\n",
        "    # Создаем веса для ВСЕХ 9 классов\n",
        "    class_weights = torch.ones(9, device=DEVICE) * 0.1  # Начальное значение 0.1 для всех\n",
        "\n",
        "    # Обновляем веса для конкретных классов (кроме 'O')\n",
        "    label2id = model.config.label2id\n",
        "    for cls, count in class_counts.items():\n",
        "        if cls != 'O' and count > 0:\n",
        "            class_weights[label2id[cls]] = 1.0 / (count + 1e-5)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
        "\n",
        "    train_dataset = NERDataset(train_texts, train_anns, tokenizer, model_config=model.config)\n",
        "    val_dataset = NERDataset(val_texts, val_anns, tokenizer, model_config=model.config)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Настройка оптимизатора и планировщика\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Обучение\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        # ... остальной код обучения ...\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        # Фаза обучения\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                labels=batch['labels']\n",
        "            )\n",
        "\n",
        "            # Используем нашу кастомную функцию потерь\n",
        "            loss = criterion(outputs.logits.view(-1, outputs.logits.shape[-1]),\n",
        "                            batch['labels'].view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Фаза валидации\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    input_ids=batch['input_ids'],\n",
        "                    attention_mask=batch['attention_mask'],\n",
        "                    labels=batch['labels']\n",
        "                )\n",
        "\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "            # Собираем предсказания и метки для метрик\n",
        "            preds = torch.argmax(outputs.logits, dim=2)\n",
        "            mask = batch['labels'] != -100  # Игнорируем метки -100\n",
        "\n",
        "            all_preds.extend(preds[mask].cpu().numpy())\n",
        "            all_labels.extend(batch['labels'][mask].cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        # Выводим метрики\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Улучшенный вывод classification_report\n",
        "        if len(all_labels) > 0 and len(all_preds) > 0:\n",
        "            try:\n",
        "                # Получаем список всех возможных классов\n",
        "                unique_labels = set(all_labels) | set(all_preds)\n",
        "                labels_list = sorted(list(unique_labels))\n",
        "\n",
        "                # Фильтруем target_names по существующим классам\n",
        "                existing_target_names = [name for i, name in enumerate(model.config.id2label.values())\n",
        "                                       if i in unique_labels or i in labels_list]\n",
        "\n",
        "                print(classification_report(\n",
        "                    all_labels,\n",
        "                    all_preds,\n",
        "                    labels=labels_list,\n",
        "                    target_names=existing_target_names,\n",
        "                    zero_division=0\n",
        "                ))\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка при выводе метрик: {str(e)}\")\n",
        "        else:\n",
        "            print(\"Нет данных для оценки метрик (возможно, все метки - 'O')\")\n",
        "\n",
        "        # Сохраняем лучшую модель\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            os.makedirs(\"./qwen_ner\", exist_ok=True)\n",
        "            model.save_pretrained(\"./qwen_ner\")\n",
        "            tokenizer.save_pretrained(\"./qwen_ner\")\n",
        "            print(\"Saved best model!\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def fix_encoding(text: str) -> str:\n",
        "    try:\n",
        "        return text.encode('latin1').decode('utf-8')\n",
        "    except:\n",
        "        return text\n",
        "\n",
        "def predict_with_qwen(text: str, model, tokenizer) -> List[Tuple[str, str]]:\n",
        "    # Предварительная очистка текста\n",
        "    text = clean_text(text)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding='max_length'\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    predictions = torch.argmax(outputs.logits, dim=2)[0]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "    results = []\n",
        "    current_word = []\n",
        "    current_label = \"O\"\n",
        "\n",
        "    for token, pred_idx in zip(tokens, predictions):\n",
        "        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:\n",
        "            continue\n",
        "\n",
        "        label = model.config.id2label.get(pred_idx.item(), \"O\")\n",
        "\n",
        "        if token.startswith('##'):\n",
        "            current_word.append(token[2:])\n",
        "        else:\n",
        "            if current_word:\n",
        "                word = ''.join(current_word)\n",
        "                results.append((word, current_label))\n",
        "            current_word = [token]\n",
        "            current_label = label\n",
        "\n",
        "    if current_word:\n",
        "        word = ''.join(current_word)\n",
        "        results.append((word, current_label))\n",
        "\n",
        "    # NEW: Постобработка результатов\n",
        "    results = [(word.replace('Ġ', '').replace('▁', ''), label)\n",
        "              for word, label in results]\n",
        "\n",
        "    # NEW: Валидация BIO-разметки\n",
        "    prev_label = None\n",
        "    clean_results = []\n",
        "    for word, label in results:\n",
        "        if label.startswith('I-') and not (\n",
        "            prev_label == label or\n",
        "            prev_label == label.replace('I-', 'B-')\n",
        "        ):\n",
        "            label = 'O'  # Исправляем некорректную I-метку\n",
        "        clean_results.append((word, label))\n",
        "        prev_label = label if label != 'O' else None\n",
        "\n",
        "    return clean_results\n",
        "\n",
        "\n",
        "def print_predictions(predictions: List[Tuple[str, str]]):\n",
        "    \"\"\"Красивый вывод предсказаний\"\"\"\n",
        "    from termcolor import colored\n",
        "\n",
        "    print(\"\\n=== Результаты распознавания ===\")\n",
        "    for word, label in predictions:\n",
        "        if label == 'O':\n",
        "            print(word, end=' ')\n",
        "        else:\n",
        "            color = {\n",
        "                'B-PER': 'green',\n",
        "                'I-PER': 'green',\n",
        "                'B-ORG': 'blue',\n",
        "                'I-ORG': 'blue',\n",
        "                'B-LOC': 'red',\n",
        "                'I-LOC': 'red'\n",
        "            }.get(label, 'yellow')\n",
        "            print(colored(word, color), end=' ')\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def main(url: str):\n",
        "    # 1. Извлекаем текст\n",
        "    text = extract_text_from_url(url)\n",
        "    if not text:\n",
        "        print(\"Не удалось извлечь текст\")\n",
        "        return\n",
        "\n",
        "    # NEW: Дополнительная очистка текста\n",
        "    text = clean_text(text)\n",
        "    print(\"\\n=== Исходный текст ===\\n\", text[:500], \"\\n\")\n",
        "\n",
        "    # 2. Аннотируем с помощью BERT\n",
        "    bert_annotations, stats = annotate_with_bert(text[:512])\n",
        "\n",
        "    if not bert_annotations:\n",
        "        print(\"Не найдено сущностей для обучения\")\n",
        "        return\n",
        "\n",
        "    # 3. Проверяем аннотации\n",
        "    if not bert_annotations:\n",
        "        print(\"Не найдено сущностей для обучения\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n=== Примеры аннотаций ===\")\n",
        "    for i, ann in enumerate(bert_annotations[:5]):\n",
        "        print(f\"{ann['word']} -> {ann['entity']} (score: {ann['score']:.2f})\")\n",
        "\n",
        "    # 4. Анализ распределения\n",
        "    print(\"\\n=== Распределение сущностей ===\")\n",
        "    for ent_type, count in stats.items():\n",
        "        print(f\"{ent_type}: {count}\")\n",
        "\n",
        "    bio_counts = analyze_class_balance(bert_annotations)\n",
        "    print(\"\\n=== BIO-распределение ===\")\n",
        "    for label in ['B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']:\n",
        "        print(f\"{label}: {bio_counts[label]}\")\n",
        "\n",
        "    # 5. Адаптация и обучение модели\n",
        "    qwen_model, qwen_tokenizer = adapt_qwen_for_ner()\n",
        "    if qwen_model is None:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        qwen_model = train_qwen(qwen_model, qwen_tokenizer,\n",
        "                              text[:2000], bert_annotations, epochs=5)\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка обучения: {e}\")\n",
        "        return\n",
        "\n",
        "    # 6. Делаем предсказания\n",
        "    text_sample = text[:512]  # Используем текст напрямую без нормализации\n",
        "    if not text_sample:\n",
        "        print(\"Текст пуст\")\n",
        "        return\n",
        "\n",
        "    # Предсказания\n",
        "    predictions = predict_with_qwen(text_sample, qwen_model, qwen_tokenizer)\n",
        "\n",
        "    # Вывод результатов\n",
        "    print_predictions(predictions)\n",
        "\n",
        "    # Детализированный вывод\n",
        "    print(\"\\n=== Детализация сущностей ===\")\n",
        "    for word, label in predictions:\n",
        "        if label != 'O':\n",
        "            print(f\"{word} -> {label}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://ru.wikipedia.org/wiki/История_Google\"  # Пример URL\n",
        "    main(url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ftfy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bub6KKJ4Pro",
        "outputId": "82fb139c-2683-47ec-802e-be41207b2857"
      },
      "id": "5bub6KKJ4Pro",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TTg16OfpKd-c",
      "metadata": {
        "id": "TTg16OfpKd-c"
      },
      "source": [
        "# FACEBOOK AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KMp-0M8qKezD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KMp-0M8qKezD",
        "outputId": "71b790d5-5f50-4618-a024-d5336aebe6a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Извлечённый текст:\n",
            "Блог / Новости Возрождение лютоволков из «Игры престолов»: научный прорыв Полина Недашковская 07 мая, 2025 • 2 мин • 144 Копировать ссылку Telegram VK WhatsApp Биотехнологическая компания Colossal Biosciences из Далласа совершила прорыв в области возрождения вымерших видов животных, воссоздав ужасных волков (dire wolves), известных широкой аудитории как «лютоволки» из «Игры престолов» . Процесс был долгим и сложным, но в итоге ученые смогли вернуть к жизни этот вид, вымерший более 12 тысяч лет н...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at FacebookAI/xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Пример аннотаций XLM-RoBERTa:\n",
            "[{'entity': 'I-MISC', 'score': np.float32(0.9999461), 'index': 4, 'word': 'И', 'start': 42, 'end': 43}, {'entity': 'I-MISC', 'score': np.float32(0.9999021), 'index': 4, 'word': 'г', 'start': 43, 'end': 44}, {'entity': 'I-MISC', 'score': np.float32(0.99977), 'index': 4, 'word': 'ры', 'start': 44, 'end': 46}, {'entity': 'I-MISC', 'score': np.float32(0.99995315), 'index': 4, 'word': '▁престол', 'start': 47, 'end': 54}, {'entity': 'I-MISC', 'score': np.float32(0.99939775), 'index': 4, 'word': 'ов', 'start': 54, 'end': 56}]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at FacebookAI/xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2/2 [01:45<00:00, 52.74s/it]\n",
            "Validating: 100%|██████████| 1/1 [00:04<00:00,  4.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: nan | Val Loss: nan\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2/2 [00:49<00:00, 24.75s/it]\n",
            "Validating: 100%|██████████| 1/1 [00:04<00:00,  4.99s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: nan | Val Loss: nan\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2/2 [00:51<00:00, 25.55s/it]\n",
            "Validating: 100%|██████████| 1/1 [00:05<00:00,  5.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: nan | Val Loss: nan\n",
            "Предсказания модели XLM-RoBERTa (первые 20 токенов):\n",
            "<s> -> O\n",
            "▁Блог -> O\n",
            "▁/ -> O\n",
            "▁Новости -> O\n",
            "▁Воз -> O\n",
            "рожден -> O\n",
            "ие -> O\n",
            "▁лют -> O\n",
            "о -> O\n",
            "вол -> O\n",
            "ков -> O\n",
            "▁из -> O\n",
            "▁« -> O\n",
            "И -> I-MISC\n",
            "г -> I-MISC\n",
            "ры -> I-MISC\n",
            "▁престол -> I-MISC\n",
            "ов -> I-MISC\n",
            "»: -> O\n",
            "▁на -> O\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "from typing import List, Dict, Tuple\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Конфигурация\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_NAME = \"FacebookAI/xlm-roberta-large-finetuned-conll03-english\"\n",
        "\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "import requests\n",
        "\n",
        "def extract_text_from_url(url: str, timeout: int = 10) -> str:\n",
        "    \"\"\"\n",
        "    Извлекает основной текстовый контент с веб-страницы по URL.\n",
        "\n",
        "    Параметры:\n",
        "        url (str): URL веб-страницы\n",
        "        timeout (int): Таймаут запроса в секундах (по умолчанию 10)\n",
        "\n",
        "    Возвращает:\n",
        "        str: Очищенный текстовый контент страницы\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Заголовки для имитации браузера\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        # Запрос с таймаутом и заголовками\n",
        "        response = requests.get(url, headers=headers, timeout=timeout)\n",
        "        response.raise_for_status()  # Проверка на ошибки HTTP\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Удаление нежелательных элементов\n",
        "        for element in soup(['script', 'style', 'nav', 'footer', 'iframe', 'noscript', 'svg']):\n",
        "            element.decompose()\n",
        "\n",
        "        # Удаление HTML-комментариев\n",
        "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
        "            comment.extract()\n",
        "\n",
        "        # Извлечение текста из основных содержательных тегов\n",
        "        text_elements = []\n",
        "        for tag in ['article', 'main', 'div', 'section', 'p']:\n",
        "            elements = soup.find_all(tag)\n",
        "            for element in elements:\n",
        "                # Проверка на содержание значимого текста\n",
        "                if len(element.get_text(strip=True)) > 50:  # Минимум 50 символов\n",
        "                    text_elements.append(element.get_text(separator=' ', strip=True))\n",
        "\n",
        "        # Если не нашли достаточно текста в специфичных тегах, берем весь body\n",
        "        if not text_elements or sum(len(t) for t in text_elements) < 500:\n",
        "            text_elements = [soup.get_text(separator=' ', strip=True)]\n",
        "\n",
        "        # Объединение и очистка текста\n",
        "        text = ' '.join(text_elements)\n",
        "\n",
        "        # Удаление лишних пробелов и переносов строк\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Ошибка при запросе к {url}: {str(e)}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Неожиданная ошибка при обработке {url}: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def annotate_with_xlm_roberta(text: str) -> List[Dict]:\n",
        "    \"\"\"Аннотирует текст с помощью предобученной модели XLM-RoBERTa.\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "\n",
        "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=0 if DEVICE.type == \"cuda\" else -1)\n",
        "    annotations = nlp(text)\n",
        "\n",
        "    # Добавляем индекс сущности в соответствии с id2label модели\n",
        "    label2id = model.config.label2id\n",
        "    for ann in annotations:\n",
        "        ann['index'] = label2id[ann['entity']]\n",
        "\n",
        "    return annotations\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    \"\"\"Кастомный Dataset для NER.\"\"\"\n",
        "    def __init__(self, texts, annotations, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.annotations = annotations\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.label_map = {\n",
        "            'O': 0,\n",
        "            'B-PER': 1,\n",
        "            'I-PER': 2,\n",
        "            'B-ORG': 3,\n",
        "            'I-ORG': 4,\n",
        "            'B-LOC': 5,\n",
        "            'I-LOC': 6,\n",
        "            'B-MISC': 7,\n",
        "            'I-MISC': 8\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        annotations = self.annotations[idx]\n",
        "\n",
        "        # Токенизация с выравниванием меток\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt',\n",
        "            return_offsets_mapping=True\n",
        "        )\n",
        "\n",
        "        # Создаем массив меток, заполненный -100 (игнорируется при вычислении потерь)\n",
        "        labels = torch.full((self.max_length,), -100, dtype=torch.long)\n",
        "\n",
        "        # Преобразуем аннотации в метки для токенов\n",
        "        offset_mapping = encoding['offset_mapping'][0]\n",
        "        for ann in annotations:\n",
        "            start, end = ann['start'], ann['end']\n",
        "            label_idx = ann['index']  # Используем предварительно сохраненный индекс\n",
        "\n",
        "            # Находим токены, которые пересекаются с аннотацией\n",
        "            for i, (token_start, token_end) in enumerate(offset_mapping):\n",
        "                if token_start >= end or token_end <= start:\n",
        "                    continue\n",
        "                if token_start != 0 or token_end != 0:  # Игнорируем специальные токены\n",
        "                    # Убедимся, что индекс метки в допустимых пределах\n",
        "                    if 0 <= label_idx < len(self.label_map):\n",
        "                        labels[i] = label_idx\n",
        "                    else:\n",
        "                        labels[i] = 0  # По умолчанию 'O'\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "def prepare_training_data(text, annotations, tokenizer, test_size=0.2):\n",
        "    \"\"\"Подготавливает данные для обучения и валидации.\"\"\"\n",
        "    # Разделяем текст на предложения (упрощенно)\n",
        "    sentences = [sent.strip() for sent in text.split('.') if len(sent.strip()) > 0]\n",
        "\n",
        "    # Сопоставляем аннотации с предложениями (упрощенно)\n",
        "    sentence_annotations = []\n",
        "    current_pos = 0\n",
        "    for sent in sentences:\n",
        "        sent_len = len(sent)\n",
        "        sent_anns = []\n",
        "        for ann in annotations:\n",
        "            if current_pos <= ann['start'] < current_pos + sent_len:\n",
        "                adjusted_ann = {\n",
        "                    'start': ann['start'] - current_pos,\n",
        "                    'end': ann['end'] - current_pos,\n",
        "                    'entity': ann['entity'],\n",
        "                    'index': ann['index']  # Сохраняем индекс сущности\n",
        "                }\n",
        "                sent_anns.append(adjusted_ann)\n",
        "        sentence_annotations.append(sent_anns)\n",
        "        current_pos += sent_len + 1  # +1 для точки\n",
        "\n",
        "    # Разделяем на train и test\n",
        "    split_idx = int(len(sentences) * (1 - test_size))\n",
        "    train_texts, train_anns = sentences[:split_idx], sentence_annotations[:split_idx]\n",
        "    val_texts, val_anns = sentences[split_idx:], sentence_annotations[split_idx:]\n",
        "\n",
        "    return train_texts, train_anns, val_texts, val_anns\n",
        "\n",
        "def train_model(model, tokenizer, text, annotations, epochs=3, batch_size=8, learning_rate=2e-5):\n",
        "    \"\"\"Функция обучения модели XLM-RoBERTa для NER.\"\"\"\n",
        "\n",
        "    # Подготовка данных\n",
        "    train_texts, train_anns, val_texts, val_anns = prepare_training_data(text, annotations, tokenizer)\n",
        "\n",
        "    train_dataset = NERDataset(train_texts, train_anns, tokenizer)\n",
        "    val_dataset = NERDataset(val_texts, val_anns, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Настройка оптимизатора и планировщика\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Обучение\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        # Фаза обучения\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                labels=batch['labels']\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Фаза валидации\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    input_ids=batch['input_ids'],\n",
        "                    attention_mask=batch['attention_mask'],\n",
        "                    labels=batch['labels']\n",
        "                )\n",
        "\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "            # Собираем предсказания и метки для метрик\n",
        "            preds = torch.argmax(outputs.logits, dim=2)\n",
        "            mask = batch['labels'] != -100  # Игнорируем метки -100\n",
        "\n",
        "            all_preds.extend(preds[mask].cpu().numpy())\n",
        "            all_labels.extend(batch['labels'][mask].cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        # Выводим метрики\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "        # print(classification_report(\n",
        "        #     all_labels,\n",
        "        #     all_preds,\n",
        "        #     target_names=list(model.config.id2label.values()),\n",
        "        #     zero_division=0\n",
        "        # ))\n",
        "\n",
        "        # Сохраняем лучшую модель\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            os.makedirs(\"./xlm_roberta_ner\", exist_ok=True)\n",
        "            model.save_pretrained(\"./xlm_roberta_ner\")\n",
        "            tokenizer.save_pretrained(\"./xlm_roberta_ner\")\n",
        "            print(\"Saved best model!\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def predict_with_model(text: str, model, tokenizer) -> List[Tuple[str, str]]:\n",
        "    \"\"\"Делает предсказания с помощью модели XLM-RoBERTa.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    predictions = torch.argmax(outputs.logits, dim=2)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    labels = [model.config.id2label[p.item()] for p in predictions[0]]\n",
        "\n",
        "    return list(zip(tokens, labels))\n",
        "\n",
        "def main(url: str):\n",
        "    # 1. Извлекаем текст с веб-страницы\n",
        "    text = extract_text_from_url(url)\n",
        "    print(f\"Извлечённый текст:\\n{text[:500]}...\\n\")\n",
        "\n",
        "    # 2. Аннотируем с помощью XLM-RoBERTa\n",
        "    annotations = annotate_with_xlm_roberta(text[:512])  # Ограничиваем длину для модели\n",
        "    print(f\"Пример аннотаций XLM-RoBERTa:\\n{annotations[:5]}\\n\")\n",
        "\n",
        "    # 3. Загружаем модель и токенизатор\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "\n",
        "    # 4. Обучаем модель (дообучение)\n",
        "    model = train_model(model, tokenizer, text[:2000], annotations, epochs=3)\n",
        "\n",
        "    # 5. Делаем предсказания\n",
        "    predictions = predict_with_model(text[:512], model, tokenizer)\n",
        "\n",
        "    # 6. Выводим результаты\n",
        "    print(\"Предсказания модели XLM-RoBERTa (первые 20 токенов):\")\n",
        "    for token, label in predictions[:20]:\n",
        "        print(f\"{token} -> {label}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://amulex.ru/daily/news/vozrozhdenie-lyutovolkov-iz-igry-prestolov-nauchnyj-proryv-bjfj4kf/?ysclid=mae5l3wify323318294\"\n",
        "    main(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P5RUEEzrPVw7",
      "metadata": {
        "id": "P5RUEEzrPVw7"
      },
      "source": [
        "⁉\n",
        "# Babelscape/wikineural-multilingual-ner\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lfIBee0kPWML",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfIBee0kPWML",
        "outputId": "84ee4fa1-a580-4fb8-e294-0570e6ade379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Извлечённый текст:\n",
            "Блог / Новости Возрождение лютоволков из «Игры престолов»: научный прорыв Полина Недашковская 07 мая, 2025 • 2 мин • 150 Копировать ссылку Telegram VK WhatsApp Биотехнологическая компания Colossal Biosciences из Далласа совершила прорыв в области возрождения вымерших видов животных, воссоздав ужасных волков (dire wolves), известных широкой аудитории как «лютоволки» из «Игры престолов» . Процесс был долгим и сложным, но в итоге ученые смогли вернуть к жизни этот вид, вымерший более 12 тысяч лет н...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Пример аннотаций WikiNeural:\n",
            "[{'entity': 'I-MISC', 'score': np.float32(0.57293886), 'index': 8, 'word': '##гр', 'start': 43, 'end': 45}, {'entity': 'I-MISC', 'score': np.float32(0.59628475), 'index': 8, 'word': '##ы', 'start': 45, 'end': 46}, {'entity': 'I-MISC', 'score': np.float32(0.5956247), 'index': 8, 'word': 'престол', 'start': 47, 'end': 54}, {'entity': 'I-MISC', 'score': np.float32(0.5244818), 'index': 8, 'word': '##ов', 'start': 54, 'end': 56}, {'entity': 'B-PER', 'score': np.float32(0.99309933), 'index': 1, 'word': 'Пол', 'start': 74, 'end': 77}]\n",
            "\n",
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2/2 [00:40<00:00, 20.09s/it]\n",
            "Validating: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: nan | Val Loss: nan\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2/2 [00:17<00:00,  8.70s/it]\n",
            "Validating: 100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: nan | Val Loss: nan\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2/2 [00:16<00:00,  8.15s/it]\n",
            "Validating: 100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: nan | Val Loss: nan\n",
            "Предсказания модели WikiNeural (первые 20 токенов):\n",
            "[CLS] -> O\n",
            "Б -> I-MISC\n",
            "##лог -> I-MISC\n",
            "/ -> O\n",
            "Ново -> I-MISC\n",
            "##сти -> I-MISC\n",
            "Во -> I-MISC\n",
            "##з -> O\n",
            "##ро -> O\n",
            "##ждение -> I-MISC\n",
            "л -> O\n",
            "##ют -> O\n",
            "##ово -> O\n",
            "##лков -> O\n",
            "из -> O\n",
            "« -> I-MISC\n",
            "И -> I-MISC\n",
            "##гр -> I-MISC\n",
            "##ы -> I-MISC\n",
            "престол -> I-MISC\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "from typing import List, Dict, Tuple\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Конфигурация\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_NAME = \"Babelscape/wikineural-multilingual-ner\"\n",
        "\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "import requests\n",
        "\n",
        "def extract_text_from_url(url: str, timeout: int = 10) -> str:\n",
        "    \"\"\"\n",
        "    Извлекает основной текстовый контент с веб-страницы по URL.\n",
        "\n",
        "    Параметры:\n",
        "        url (str): URL веб-страницы\n",
        "        timeout (int): Таймаут запроса в секундах (по умолчанию 10)\n",
        "\n",
        "    Возвращает:\n",
        "        str: Очищенный текстовый контент страницы\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Заголовки для имитации браузера\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        # Запрос с таймаутом и заголовками\n",
        "        response = requests.get(url, headers=headers, timeout=timeout)\n",
        "        response.raise_for_status()  # Проверка на ошибки HTTP\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Удаление нежелательных элементов\n",
        "        for element in soup(['script', 'style', 'nav', 'footer', 'iframe', 'noscript', 'svg']):\n",
        "            element.decompose()\n",
        "\n",
        "        # Удаление HTML-комментариев\n",
        "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
        "            comment.extract()\n",
        "\n",
        "        # Извлечение текста из основных содержательных тегов\n",
        "        text_elements = []\n",
        "        for tag in ['article', 'main', 'div', 'section', 'p']:\n",
        "            elements = soup.find_all(tag)\n",
        "            for element in elements:\n",
        "                # Проверка на содержание значимого текста\n",
        "                if len(element.get_text(strip=True)) > 50:  # Минимум 50 символов\n",
        "                    text_elements.append(element.get_text(separator=' ', strip=True))\n",
        "\n",
        "        # Если не нашли достаточно текста в специфичных тегах, берем весь body\n",
        "        if not text_elements or sum(len(t) for t in text_elements) < 500:\n",
        "            text_elements = [soup.get_text(separator=' ', strip=True)]\n",
        "\n",
        "        # Объединение и очистка текста\n",
        "        text = ' '.join(text_elements)\n",
        "\n",
        "        # Удаление лишних пробелов и переносов строк\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Ошибка при запросе к {url}: {str(e)}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Неожиданная ошибка при обработке {url}: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def annotate_with_wikineural(text: str) -> List[Dict]:\n",
        "    \"\"\"Аннотирует текст с помощью модели WikiNeural.\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "\n",
        "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=0 if DEVICE.type == \"cuda\" else -1)\n",
        "    annotations = nlp(text)\n",
        "\n",
        "    # Добавляем индекс сущности в соответствии с id2label модели\n",
        "    label2id = model.config.label2id\n",
        "    for ann in annotations:\n",
        "        ann['index'] = label2id[ann['entity']]\n",
        "\n",
        "    return annotations\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    \"\"\"Кастомный Dataset для NER.\"\"\"\n",
        "    def __init__(self, texts, annotations, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.annotations = annotations\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        annotations = self.annotations[idx]\n",
        "\n",
        "        # Токенизация с выравниванием меток\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt',\n",
        "            return_offsets_mapping=True\n",
        "        )\n",
        "\n",
        "        # Создаем массив меток, заполненный -100 (игнорируется при вычислении потерь)\n",
        "        labels = torch.full((self.max_length,), -100, dtype=torch.long)\n",
        "\n",
        "        # Преобразуем аннотации в метки для токенов\n",
        "        offset_mapping = encoding['offset_mapping'][0]\n",
        "        for ann in annotations:\n",
        "            start, end = ann['start'], ann['end']\n",
        "            label_idx = ann['index']\n",
        "\n",
        "            # Находим токены, которые пересекаются с аннотацией\n",
        "            for i, (token_start, token_end) in enumerate(offset_mapping):\n",
        "                if token_start >= end or token_end <= start:\n",
        "                    continue\n",
        "                if token_start != 0 or token_end != 0:  # Игнорируем специальные токены\n",
        "                    labels[i] = label_idx\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "def prepare_training_data(text, annotations, tokenizer, test_size=0.2):\n",
        "    \"\"\"Подготавливает данные для обучения и валидации.\"\"\"\n",
        "    # Разделяем текст на предложения (упрощенно)\n",
        "    sentences = [sent.strip() for sent in text.split('.') if len(sent.strip()) > 0]\n",
        "\n",
        "    # Сопоставляем аннотации с предложениями (упрощенно)\n",
        "    sentence_annotations = []\n",
        "    current_pos = 0\n",
        "    for sent in sentences:\n",
        "        sent_len = len(sent)\n",
        "        sent_anns = []\n",
        "        for ann in annotations:\n",
        "            if current_pos <= ann['start'] < current_pos + sent_len:\n",
        "                adjusted_ann = {\n",
        "                    'start': ann['start'] - current_pos,\n",
        "                    'end': ann['end'] - current_pos,\n",
        "                    'entity': ann['entity'],\n",
        "                    'index': ann['index']\n",
        "                }\n",
        "                sent_anns.append(adjusted_ann)\n",
        "        sentence_annotations.append(sent_anns)\n",
        "        current_pos += sent_len + 1  # +1 для точки\n",
        "\n",
        "    # Разделяем на train и test\n",
        "    split_idx = int(len(sentences) * (1 - test_size))\n",
        "    train_texts, train_anns = sentences[:split_idx], sentence_annotations[:split_idx]\n",
        "    val_texts, val_anns = sentences[split_idx:], sentence_annotations[split_idx:]\n",
        "\n",
        "    return train_texts, train_anns, val_texts, val_anns\n",
        "\n",
        "def train_model(model, tokenizer, text, annotations, epochs=3, batch_size=8, learning_rate=2e-5):\n",
        "    \"\"\"Функция обучения модели WikiNeural для NER.\"\"\"\n",
        "\n",
        "    # Подготовка данных\n",
        "    train_texts, train_anns, val_texts, val_anns = prepare_training_data(text, annotations, tokenizer)\n",
        "\n",
        "    train_dataset = NERDataset(train_texts, train_anns, tokenizer)\n",
        "    val_dataset = NERDataset(val_texts, val_anns, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Настройка оптимизатора и планировщика\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Обучение\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        # Фаза обучения\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                labels=batch['labels']\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Фаза валидации\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    input_ids=batch['input_ids'],\n",
        "                    attention_mask=batch['attention_mask'],\n",
        "                    labels=batch['labels']\n",
        "                )\n",
        "\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "            # Собираем предсказания и метки для метрик\n",
        "            preds = torch.argmax(outputs.logits, dim=2)\n",
        "            mask = batch['labels'] != -100  # Игнорируем метки -100\n",
        "\n",
        "            all_preds.extend(preds[mask].cpu().numpy())\n",
        "            all_labels.extend(batch['labels'][mask].cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        # Выводим метрики\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "        if len(all_labels) > 0:\n",
        "            print(classification_report(\n",
        "                all_labels,\n",
        "                all_preds,\n",
        "                target_names=list(model.config.id2label.values()),\n",
        "                zero_division=0\n",
        "            ))\n",
        "\n",
        "        # Сохраняем лучшую модель\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            os.makedirs(\"./wikineural_ner\", exist_ok=True)\n",
        "            model.save_pretrained(\"./wikineural_ner\")\n",
        "            tokenizer.save_pretrained(\"./wikineural_ner\")\n",
        "            print(\"Saved best model!\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def predict_with_model(text: str, model, tokenizer) -> List[Tuple[str, str]]:\n",
        "    \"\"\"Делает предсказания с помощью модели WikiNeural.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    predictions = torch.argmax(outputs.logits, dim=2)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    labels = [model.config.id2label[p.item()] for p in predictions[0]]\n",
        "\n",
        "    return list(zip(tokens, labels))\n",
        "\n",
        "def main(url: str):\n",
        "    # 1. Извлекаем текст с веб-страницы\n",
        "    text = extract_text_from_url(url)\n",
        "    print(f\"Извлечённый текст:\\n{text[:500]}...\\n\")\n",
        "\n",
        "    # 2. Аннотируем с помощью WikiNeural\n",
        "    annotations = annotate_with_wikineural(text[:512])  # Ограничиваем длину для модели\n",
        "    print(f\"Пример аннотаций WikiNeural:\\n{annotations[:5]}\\n\")\n",
        "\n",
        "    # 3. Загружаем модель и токенизатор\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "\n",
        "    # 4. Обучаем модель (дообучение)\n",
        "    model = train_model(model, tokenizer, text[:2000], annotations, epochs=3)\n",
        "\n",
        "    # 5. Делаем предсказания\n",
        "    predictions = predict_with_model(text[:512], model, tokenizer)\n",
        "\n",
        "    # 6. Выводим результаты\n",
        "    print(\"Предсказания модели WikiNeural (первые 20 токенов):\")\n",
        "    for token, label in predictions[:20]:\n",
        "        print(f\"{token} -> {label}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://amulex.ru/daily/news/vozrozhdenie-lyutovolkov-iz-igry-prestolov-nauchnyj-proryv-bjfj4kf/?ysclid=mae5l3wify323318294\"  # Пример URL\n",
        "    main(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hb8_BoioyWm_",
      "metadata": {
        "id": "hb8_BoioyWm_"
      },
      "source": [
        "# STREAMLIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9n-2UEIKexMC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n-2UEIKexMC",
        "outputId": "a57654c0-63be-49c2-b9eb-d00d4d99c28e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Using cached streamlit-1.45.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting blinker\n",
            "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting altair<6,>=4.0 (from streamlit)\n",
            "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting cachetools<6,>=4.0 (from streamlit)\n",
            "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting click<9,>=7.0 (from streamlit)\n",
            "  Using cached click-8.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting numpy<3,>=1.23 (from streamlit)\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting packaging<25,>=20 (from streamlit)\n",
            "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pandas<3,>=1.4.0 (from streamlit)\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting pillow<12,>=7.1.0 (from streamlit)\n",
            "  Using cached pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting protobuf<7,>=3.20 (from streamlit)\n",
            "  Using cached protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting pyarrow>=7.0 (from streamlit)\n",
            "  Using cached pyarrow-20.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting requests<3,>=2.27 (from streamlit)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
            "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting toml<2,>=0.10.1 (from streamlit)\n",
            "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-extensions<5,>=4.4.0 (from streamlit)\n",
            "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting tornado<7,>=6.0.3 (from streamlit)\n",
            "  Using cached tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting jinja2 (from altair<6,>=4.0->streamlit)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
            "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
            "  Using cached narwhals-1.38.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas<3,>=1.4.0->streamlit)\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas<3,>=1.4.0->streamlit)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas<3,>=1.4.0->streamlit)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.27->streamlit)\n",
            "  Using cached charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2.27->streamlit)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.27->streamlit)\n",
            "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2.27->streamlit)\n",
            "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->altair<6,>=4.0->streamlit)\n",
            "  Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=22.2.0 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
            "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
            "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
            "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
            "  Using cached rpds_py-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Using cached streamlit-1.45.0-py3-none-any.whl (9.9 MB)\n",
            "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
            "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
            "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Using cached click-8.2.0-py3-none-any.whl (102 kB)\n",
            "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
            "Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "Using cached pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "Using cached protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "Using cached pyarrow-20.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.3 MB)\n",
            "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Using cached tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (437 kB)\n",
            "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Using cached charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
            "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
            "Using cached narwhals-1.38.2-py3-none-any.whl (338 kB)\n",
            "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
            "Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
            "Using cached rpds_py-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (389 kB)\n",
            "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pytz, watchdog, urllib3, tzdata, typing-extensions, tornado, toml, tenacity, smmap, six, rpds-py, pyarrow, protobuf, pillow, packaging, numpy, narwhals, MarkupSafe, idna, click, charset-normalizer, certifi, cachetools, blinker, attrs, requests, referencing, python-dateutil, jinja2, gitdb, pydeck, pandas, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 6.30.2 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.30.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.30.2 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 6.30.2 which is incompatible.\n",
            "pylibcudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\n",
            "google-cloud-pubsub 2.25.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.30.2 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.30.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 altair-5.5.0 attrs-25.3.0 blinker-1.9.0 cachetools-5.5.2 certifi-2025.4.26 charset-normalizer-3.4.2 click-8.2.0 gitdb-4.0.12 gitpython-3.1.44 idna-3.10 jinja2-3.1.6 jsonschema-4.23.0 jsonschema-specifications-2025.4.1 narwhals-1.38.2 numpy-2.2.5 packaging-24.2 pandas-2.2.3 pillow-11.2.1 protobuf-6.30.2 pyarrow-20.0.0 pydeck-0.9.1 python-dateutil-2.9.0.post0 pytz-2025.2 referencing-0.36.2 requests-2.32.3 rpds-py-0.24.0 six-1.17.0 smmap-5.0.2 streamlit-1.45.0 tenacity-9.1.2 toml-0.10.2 tornado-6.4.2 typing-extensions-4.13.2 tzdata-2025.2 urllib3-2.4.0 watchdog-6.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "cachetools",
                  "certifi",
                  "charset_normalizer",
                  "dateutil",
                  "markupsafe",
                  "requests",
                  "six",
                  "streamlit",
                  "tornado"
                ]
              },
              "id": "f3fcf64ef2a846f3ab4f8668bb4c2d0c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install streamlit --ignore-installed blinker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LJ5okGncxCoS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LJ5okGncxCoS",
        "outputId": "6a72311c-76e6-4865-c1c7-c0c8e7eee8ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.5)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.3)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.30.2)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (20.0.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.38.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        " pip install streamlit transformers torch beautifulsoup4 requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MWoVRiRICxLi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "MWoVRiRICxLi",
        "outputId": "895d4b23-5ecb-46a5-b1f0-628f6ca9c72e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.auto.tokenization_auto because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name '_center' from 'numpy._core.umath' (/usr/local/lib/python3.11/dist-packages/numpy/_core/umath.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidate_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAssistantVocabTranslatorCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/candidate_generator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_sklearn_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0m\u001b[1;32m      6\u001b[0m                        \u001b[0mget_sum_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_sputils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ulong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_namespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_docscrape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/strings/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__all__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/strings.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from numpy._core.umath import (\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0misalpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_center' from 'numpy._core.umath' (/usr/local/lib/python3.11/dist-packages/numpy/_core/umath.py)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_decoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoderDecoderConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mauto_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_LazyAutoMapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m from .configuration_auto import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name '_center' from 'numpy._core.umath' (/usr/local/lib/python3.11/dist-packages/numpy/_core/umath.py)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-12d9990849ea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1956\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.auto.tokenization_auto because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name '_center' from 'numpy._core.umath' (/usr/local/lib/python3.11/dist-packages/numpy/_core/umath.py)"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Настройки страницы\n",
        "st.set_page_config(\n",
        "    page_title=\"NER с Qwen\",\n",
        "    page_icon=\":mag:\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Заголовок приложения\n",
        "st.title(\"Извлечение именованных сущностей (NER) с Qwen\")\n",
        "st.markdown(\"\"\"\n",
        "Это приложение анализирует текст с веб-страницы и извлекает именованные сущности с помощью модели Qwen.\n",
        "\"\"\")\n",
        "\n",
        "# Функция для получения текста с сайта\n",
        "@st.cache_data(show_spinner=False)\n",
        "def get_website_text(url):\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Удаляем ненужные элементы\n",
        "        for element in soup(['script', 'style', 'meta', 'link', 'nav', 'footer']):\n",
        "            element.decompose()\n",
        "\n",
        "        return ' '.join(soup.stripped_strings)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Ошибка при получении текста: {e}\")\n",
        "        return None\n",
        "\n",
        "# Загрузка модели с индикатором прогресса\n",
        "@st.cache_resource(show_spinner=\"Загрузка модели...\")\n",
        "def load_model():\n",
        "    model_name = \"sberbank-ai/ruRoberta-large\" # или \"Qwen/Qwen2.5-32B-Instruct\"\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        st.error(f\"Ошибка загрузки модели: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Основной интерфейс\n",
        "def main():\n",
        "    # Сайдбар с настройками\n",
        "    with st.sidebar:\n",
        "        st.header(\"Настройки\")\n",
        "        url = st.text_input(\n",
        "            \"Введите URL страницы:\",\n",
        "            value=\"https://ru.wikipedia.org/wiki/История_Google\"\n",
        "        )\n",
        "        max_length = st.slider(\"Максимальная длина текста:\", 100, 2000, 512)\n",
        "        analyze_button = st.button(\"Анализировать текст\")\n",
        "\n",
        "    if analyze_button and url:\n",
        "        with st.spinner(\"Получаем текст с сайта...\"):\n",
        "            text = get_website_text(url)\n",
        "\n",
        "        if text:\n",
        "            # Обрезаем текст до выбранной длины\n",
        "            text = text[:max_length]\n",
        "\n",
        "            st.subheader(\"Извлеченный текст\")\n",
        "            st.text_area(\"Текст\", text, height=200)\n",
        "\n",
        "            with st.spinner(\"Анализируем текст...\"):\n",
        "                tokenizer, model = load_model()\n",
        "\n",
        "                if model:\n",
        "                    # Токенизация и предсказание\n",
        "                    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model(**inputs)\n",
        "\n",
        "                    # Получаем предсказания\n",
        "                    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "                    predicted_tokens = tokenizer.batch_decode(predictions[0])\n",
        "\n",
        "                    # Отображаем результаты\n",
        "                    st.subheader(\"Результаты анализа\")\n",
        "\n",
        "                    col1, col2 = st.columns(2)\n",
        "\n",
        "                    with col1:\n",
        "                        st.markdown(\"**Оригинальные токены:**\")\n",
        "                        st.write(tokenizer.tokenize(text)[:50])  # Показываем первые 50 токенов\n",
        "\n",
        "                    with col2:\n",
        "                        st.markdown(\"**Предсказанные токены:**\")\n",
        "                        st.write(predicted_tokens[:50])\n",
        "\n",
        "                    # Дополнительная информация\n",
        "                    with st.expander(\"Технические детали\"):\n",
        "                        st.write(f\"Размер модели: {sum(p.numel() for p in model.parameters()):,} параметров\")\n",
        "                        st.write(f\"Длина входного текста: {len(text)} символов\")\n",
        "                        st.write(f\"Количество токенов: {inputs.input_ids.shape[1]}\")\n",
        "                else:\n",
        "                    st.error(\"Не удалось загрузить модель\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p4m_keqjFzkq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4m_keqjFzkq",
        "outputId": "3d3a0e6d-62ee-4e6e-9f19-c60c3f08beaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.188.45.237:8501\u001b[0m\n",
            "\u001b[0m\n",
            "https://amulex.ru/daily/news/vozrozhdenie-lyutovolkov-iz-igry-prestolov-nauchnyj-proryv-bjfj4kf/?ysclid=mae5l3wify323318294\n",
            "http://localhost:8501\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NzUhkQD0ZsJk",
      "metadata": {
        "id": "NzUhkQD0ZsJk"
      },
      "source": [
        "# FASTAPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TkTmPGz4yomg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TkTmPGz4yomg",
        "outputId": "7e79e941-20d3-41f7-e95c-c8153413070f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.12 starlette-0.46.2 uvicorn-0.34.2\n"
          ]
        }
      ],
      "source": [
        "pip install fastapi uvicorn requests beautifulsoup4 torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3pAdSbOqAEYx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3pAdSbOqAEYx",
        "outputId": "8d23b488-f85d-4a9d-d625-d3f3e235a3e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.7-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.7-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.7\n"
          ]
        }
      ],
      "source": [
        "pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_k-VOm2dF2OC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k-VOm2dF2OC",
        "outputId": "be6b0c00-2cef-4b2d-9f74-f0f7a19e83ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error loading model: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './qwen_ner'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Will watch for changes in these directories: ['/content']\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "INFO:     Started reloader process [1901] using StatReload\n",
            "INFO:     Stopping reloader process [1901]\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from typing import List, Dict, Optional\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app = FastAPI(title=\"NER with Qwen\", version=\"1.0\")\n",
        "\n",
        "# Конфигурация\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_PATH = \"./qwen_ner\"  # Путь к сохраненной модели\n",
        "\n",
        "# Загрузка модели и токенизатора\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH).to(DEVICE)\n",
        "    print(\"Model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "\n",
        "class TextRequest(BaseModel):\n",
        "    text: str\n",
        "    return_entities: Optional[bool] = True\n",
        "\n",
        "class URLRequest(BaseModel):\n",
        "    url: str\n",
        "    return_entities: Optional[bool] = True\n",
        "\n",
        "class Entity(BaseModel):\n",
        "    word: str\n",
        "    entity: str\n",
        "    start: int\n",
        "    end: int\n",
        "\n",
        "class NERResponse(BaseModel):\n",
        "    tokens: List[str]\n",
        "    labels: List[str]\n",
        "    entities: Optional[List[Entity]] = None\n",
        "    model: str = \"Qwen-NER\"\n",
        "\n",
        "def extract_text_from_url(url: str) -> str:\n",
        "    \"\"\"Извлекает текст с веб-страницы по URL.\"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Удаляем ненужные элементы\n",
        "        for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"head\", \"meta\"]):\n",
        "            element.decompose()\n",
        "\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=f\"Error extracting text from URL: {str(e)}\")\n",
        "\n",
        "def predict_entities(text: str) -> NERResponse:\n",
        "    \"\"\"Делает предсказания NER для текста.\"\"\"\n",
        "    if model is None or tokenizer is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
        "\n",
        "    try:\n",
        "        # Токенизация и предсказание\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Обработка результатов\n",
        "        predictions = torch.argmax(outputs.logits, dim=2)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "        labels = [model.config.id2label[p.item()] for p in predictions[0]]\n",
        "\n",
        "        # Собираем сущности\n",
        "        entities = []\n",
        "        current_entity = None\n",
        "\n",
        "        for i, (token, label) in enumerate(zip(tokens, labels)):\n",
        "            if label.startswith(\"B-\"):\n",
        "                if current_entity:\n",
        "                    entities.append(current_entity)\n",
        "                current_entity = {\n",
        "                    \"word\": token,\n",
        "                    \"entity\": label[2:],\n",
        "                    \"start\": i,\n",
        "                    \"end\": i\n",
        "                }\n",
        "            elif label.startswith(\"I-\") and current_entity and label[2:] == current_entity[\"entity\"]:\n",
        "                current_entity[\"word\"] += \" \" + token\n",
        "                current_entity[\"end\"] = i\n",
        "            else:\n",
        "                if current_entity:\n",
        "                    entities.append(current_entity)\n",
        "                    current_entity = None\n",
        "\n",
        "        if current_entity:\n",
        "            entities.append(current_entity)\n",
        "\n",
        "        # Преобразуем сущности в формат с позициями в тексте\n",
        "        char_pos = 0\n",
        "        text_entities = []\n",
        "        offset_mapping = inputs[\"offset_mapping\"][0].cpu().numpy()\n",
        "\n",
        "        for entity in entities:\n",
        "            start_token, end_token = entity[\"start\"], entity[\"end\"]\n",
        "            start_pos = offset_mapping[start_token][0].item()\n",
        "            end_pos = offset_mapping[end_token][1].item()\n",
        "\n",
        "            text_entities.append(Entity(\n",
        "                word=entity[\"word\"],\n",
        "                entity=entity[\"entity\"],\n",
        "                start=start_pos,\n",
        "                end=end_pos\n",
        "            ))\n",
        "\n",
        "        return NERResponse(\n",
        "            tokens=tokens,\n",
        "            labels=labels,\n",
        "            entities=text_entities\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Prediction error: {str(e)}\")\n",
        "\n",
        "@app.post(\"/predict/text\", response_model=NERResponse)\n",
        "async def predict_from_text(request: TextRequest):\n",
        "    \"\"\"API endpoint для обработки текста.\"\"\"\n",
        "    result = predict_entities(request.text)\n",
        "    if not request.return_entities:\n",
        "        result.entities = None\n",
        "    return result\n",
        "\n",
        "@app.post(\"/predict/url\", response_model=NERResponse)\n",
        "async def predict_from_url(request: URLRequest):\n",
        "    \"\"\"API endpoint для обработки URL.\"\"\"\n",
        "    text = extract_text_from_url(request.url)\n",
        "    result = predict_entities(text)\n",
        "    if not request.return_entities:\n",
        "        result.entities = None\n",
        "    return result\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    \"\"\"Проверка статуса сервиса.\"\"\"\n",
        "    return {\n",
        "        \"status\": \"OK\" if model and tokenizer else \"Error\",\n",
        "        \"device\": str(DEVICE),\n",
        "        \"model_loaded\": bool(model)\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(\"__main__:app\", host=\"0.0.0.0\", port=8000, reload=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rlvv7txQA6qs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Rlvv7txQA6qs",
        "outputId": "8027e6a2-d419-45b5-d00d-d5da52506b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastapi[standard] in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (2.11.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (4.13.2)\n",
            "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (0.28.1)\n",
            "Requirement already satisfied: jinja2>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]) (3.1.6)\n",
            "Collecting python-multipart>=0.0.18 (from fastapi[standard])\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard])\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (0.34.2)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard])\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]) (3.10)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (0.15.3)\n",
            "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading rich_toolkit-0.14.5-py3-none-any.whl.metadata (999 bytes)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.1.5->fastapi[standard]) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[standard]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[standard]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[standard]) (0.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.12.0->uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (8.1.8)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (6.0.2)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard])\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]) (15.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->fastapi[standard]) (1.3.1)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (0.1.2)\n",
            "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading rich_toolkit-0.14.5-py3-none-any.whl (24 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvloop, python-multipart, python-dotenv, httptools, dnspython, watchfiles, email-validator, rich-toolkit, fastapi-cli\n",
            "Successfully installed dnspython-2.7.0 email-validator-2.2.0 fastapi-cli-0.0.7 httptools-0.6.4 python-dotenv-1.1.0 python-multipart-0.0.20 rich-toolkit-0.14.5 uvloop-0.21.0 watchfiles-1.0.5\n"
          ]
        }
      ],
      "source": [
        "pip install \"fastapi[standard]\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "egpHitQzyzkB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egpHitQzyzkB",
        "outputId": "d3ab4519-ce27-4035-c678-2a8c3f904455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "curl: (7) Failed to connect to localhost port 8000 after 0 ms: Connection refused\n"
          ]
        }
      ],
      "source": [
        "# примеры запросов, обработка текста\n",
        "!curl -X POST \"http://localhost:8000/predict/url\" \\\n",
        "-H \"Content-Type: application/json\" \\\n",
        "-d '{\"url\": \"https://ru.wikipedia.org/wiki/История_Google\", \"return_entities\": true}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Os_Qd-vlyzUs",
      "metadata": {
        "id": "Os_Qd-vlyzUs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f1fe27e3860c4cf380107fb67d43e2f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_415f1d3040fb4902aeb231047ac7fa70",
              "IPY_MODEL_307a489504ef401a9ac97224e10a66bc",
              "IPY_MODEL_28a71f72e8b54221a4af5863f5e8597b"
            ],
            "layout": "IPY_MODEL_0b98f7417dff428fbba978a6307ced56"
          }
        },
        "415f1d3040fb4902aeb231047ac7fa70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3be9576eebf46fba1eab3d0384a8a80",
            "placeholder": "​",
            "style": "IPY_MODEL_4eb63505ad5d4248987bc9d1ee304069",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "307a489504ef401a9ac97224e10a66bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbcd4e8cb8ab48d69d2c8f28f01ed159",
            "max": 17,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_321ffc2d757443b48e6988788de84f6e",
            "value": 17
          }
        },
        "28a71f72e8b54221a4af5863f5e8597b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_018b70395c624efc8e80139ce7dacd21",
            "placeholder": "​",
            "style": "IPY_MODEL_35b504a135fb4f62b5e4fa02779a8e8a",
            "value": " 17/17 [01:44&lt;00:00,  5.33s/it]"
          }
        },
        "0b98f7417dff428fbba978a6307ced56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3be9576eebf46fba1eab3d0384a8a80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eb63505ad5d4248987bc9d1ee304069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbcd4e8cb8ab48d69d2c8f28f01ed159": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "321ffc2d757443b48e6988788de84f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "018b70395c624efc8e80139ce7dacd21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35b504a135fb4f62b5e4fa02779a8e8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}